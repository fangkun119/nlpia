# Ch 10. Sequence-to-sequence models and attention

## 10.1. Encoder-decoder architecture

### 10.1.1. Decoding thought

### 10.1.2. Look familiar?

### 10.1.3. Sequence-to-sequence conversation

### 10.1.4. LSTM review

## 10.2. Assembling a sequence-to-sequence pipeline

### 10.2.1. Preparing your dataset for the sequence-to-sequence training

### 10.2.2. Sequence-to-sequence model in Keras

### 10.2.3. Sequence encoder

### 10.2.4. Thought decoder

### 10.2.5. Assembling the sequence-to-sequence network

## 10.3. Training the sequence-to-sequence network

### 10.3.1. Generate output sequences

## 10.4. Building a chatbot using sequence-to-sequence networks

### 10.4.1. Preparing the corpus for your training

### 10.4.2. Building your character dictionary

### 10.4.3. Generate one-hot encoded training sets

### 10.4.4. Train your sequence-to-sequence chatbot

### 10.4.5. Assemble the model for sequence generation

### 10.4.6. Predicting a sequence

### 10.4.7. Generating a response

### 10.4.8. Converse with your chatbot

## 10.5. Enhancements

### 10.5.1. Reduce training complexity with bucketing

### 10.5.2. Paying attention

## 10.6. In the real world

## Appendix: Code
